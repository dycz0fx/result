 benchmarks to run Iallreduce 
#------------------------------------------------------------
#    Intel (R) MPI Benchmarks 4.0 Update 1, MPI-NBC part  
#------------------------------------------------------------
# Date                  : Wed Oct  5 23:59:03 2016
# Machine               : x86_64
# System                : Linux
# Release               : 4.7.5-1.el6.elrepo.x86_64
# Version               : #1 SMP Sat Sep 24 11:59:14 EDT 2016
# MPI Version           : 3.0
# MPI Thread Environment: MPI_THREAD_MULTIPLE


# New default behavior from Version 3.2 on:

# the number of iterations per message size is cut down 
# dynamically when a certain run time (per message size sample) 
# is expected to be exceeded. Time limit is defined by variable 
# "SECS_PER_SAMPLE" (=> IMB_settings.h) 
# or through the flag => -time 
  


# Calling sequence was: 

# ./IMB-NBC Iallreduce -npmin 16 -iter 1000 -time 100
#          

# Minimum message length in bytes:   0
# Maximum message length in bytes:   4194304
#
# MPI_Datatype                   :   MPI_BYTE 
# MPI_Datatype for reductions    :   MPI_FLOAT
# MPI_Op                         :   MPI_SUM  
#
#

# List of Benchmarks to run:

# Iallreduce

#------------------------------------------------------------------------------------------
# Benchmarking Iallreduce 
# #processes = 16 
#------------------------------------------------------------------------------------------
       #bytes #repetitions t_ovrl[usec] t_pure[usec]  t_CPU[usec]   overlap[%]      defects
            0          992        67.31        48.56        48.75        61.54         0.00
            4          992        69.92        51.57        51.76        64.57         0.00
            8          992        71.12        52.37        52.50        64.28         0.00
           16          992        71.79        52.75        52.94        64.04         0.00
           32          992        91.97        70.99        71.30        70.57         0.00
           64          992        77.89        56.97        57.02        63.30         0.00
          128          992        79.80        57.46        57.61        61.22         0.00
          256          992       100.43        68.65        68.88        53.86         0.00
          512          992       107.85        76.04        76.33        58.32         0.00
         1024          992       127.63        89.26        89.49        57.12         0.00
         2048          992       174.05       120.61       121.10        55.87         0.00
         4096          976       254.28       176.73       177.27        56.26         0.00
         8192          960       377.53       250.16       251.48        49.35         0.00
        16384          928       612.61       392.50       394.03        44.14         0.00
        32768          880      1073.21       662.17       667.30        38.40         0.00
        65536          800      1983.94      1197.05      1200.39        34.45         0.00
       131072          656      3745.24      2217.94      2219.24        31.18         0.00
       262144          496      7257.94      4235.09      4241.78        28.74         0.00
       524288          320     14317.53      8351.24      8369.43        28.71         0.00
      1048576          192     28757.06     16788.99     16795.91        28.74         0.00
      2097152           96     56235.31     32037.83     32069.88        24.55         0.00
      4194304           48    114970.15     65360.69     65381.96        24.12         0.00


!!!!  ALL BENCHMARKS SUCCESSFUL !!!! 



# All processes entering MPI_Finalize

