 benchmarks to run Iallreduce 
#------------------------------------------------------------
#    Intel (R) MPI Benchmarks 4.0 Update 1, MPI-NBC part  
#------------------------------------------------------------
# Date                  : Mon Oct 10 11:25:29 2016
# Machine               : x86_64
# System                : Linux
# Release               : 4.7.5-1.el6.elrepo.x86_64
# Version               : #1 SMP Sat Sep 24 11:59:14 EDT 2016
# MPI Version           : 3.0
# MPI Thread Environment: MPI_THREAD_MULTIPLE


# New default behavior from Version 3.2 on:

# the number of iterations per message size is cut down 
# dynamically when a certain run time (per message size sample) 
# is expected to be exceeded. Time limit is defined by variable 
# "SECS_PER_SAMPLE" (=> IMB_settings.h) 
# or through the flag => -time 
  


# Calling sequence was: 

# ./IMB-NBC Iallreduce -npmin 16 -iter 1000 -time 100
#          

# Minimum message length in bytes:   0
# Maximum message length in bytes:   4194304
#
# MPI_Datatype                   :   MPI_BYTE 
# MPI_Datatype for reductions    :   MPI_FLOAT
# MPI_Op                         :   MPI_SUM  
#
#

# List of Benchmarks to run:

# Iallreduce

#------------------------------------------------------------------------------------------
# Benchmarking Iallreduce 
# #processes = 16 
#------------------------------------------------------------------------------------------
       #bytes #repetitions t_ovrl[usec] t_pure[usec]  t_CPU[usec]   overlap[%]      defects
            0          992        68.52        49.81        49.94        62.53         0.00
            4          992        68.79        50.49        50.62        63.83         0.00
            8          992        71.04        52.00        52.28        63.58         0.00
           16          992        70.01        50.86        50.97        62.43         0.00
           32          992        92.54        69.40        69.71        66.80         0.00
           64          992        79.54        58.42        58.59        63.96         0.00
          128          992        82.06        59.96        60.14        63.25         0.00
          256          992        98.86        69.68        70.00        58.32         0.00
          512          992       110.21        78.85        79.13        60.36         0.00
         1024          992       129.77        90.48        90.77        56.71         0.00
         2048          992       174.70       119.30       119.82        53.76         0.00
         4096          976       256.44       179.07       179.79        56.97         0.00
         8192          960       378.41       250.60       251.42        49.16         0.00
        16384          928       610.17       391.27       393.18        44.32         0.00
        32768          880      1069.29       659.85       663.19        38.26         0.00
        65536          800      1983.17      1195.55      1198.36        34.27         0.00
       131072          656      3753.20      2227.64      2232.19        31.66         0.00
       262144          496      7276.78      4249.63      4259.59        28.93         0.00
       524288          320     14292.44      8328.21      8347.63        28.55         0.00
      1048576          192     28675.97     16693.09     16717.51        28.32         0.00
      2097152           96     56164.42     31979.95     31999.98        24.42         0.00
      4194304           48    117718.09     66856.31     66890.32        23.96         0.00


!!!!  ALL BENCHMARKS SUCCESSFUL !!!! 



# All processes entering MPI_Finalize

