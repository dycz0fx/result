 benchmarks to run Ibcast 
#------------------------------------------------------------
#    Intel (R) MPI Benchmarks 4.0 Update 1, MPI-NBC part  
#------------------------------------------------------------
# Date                  : Sun Aug  9 22:33:10 2015
# Machine               : x86_64
# System                : Linux
# Release               : 4.0.4-1.el6.elrepo.x86_64
# Version               : #1 SMP Mon May 18 12:45:35 EDT 2015
# MPI Version           : 3.0
# MPI Thread Environment: 

# New default behavior from Version 3.2 on:

# the number of iterations per message size is cut down 
# dynamically when a certain run time (per message size sample) 
# is expected to be exceeded. Time limit is defined by variable 
# "SECS_PER_SAMPLE" (=> IMB_settings.h) 
# or through the flag => -time 
  


# Calling sequence was: 

# /home/dycz0fx/program/imb/src/IMB-NBC Ibcast -msglen /home/dycz0fx/Lengths -npmin 8

# Message lengths were user defined
#
# MPI_Datatype                   :   MPI_BYTE 
# MPI_Datatype for reductions    :   MPI_FLOAT
# MPI_Op                         :   MPI_SUM  
#
#

# List of Benchmarks to run:

# Ibcast

#-----------------------------------------------------------------------------
# Benchmarking Ibcast 
# #processes = 8 
# ( 8 additional processes waiting in MPI_Barrier)
#-----------------------------------------------------------------------------
       #bytes #repetitions t_ovrl[usec] t_pure[usec]  t_CPU[usec]   overlap[%]
          512         1000       300.85       154.50       157.74         7.22
         1024         1000       325.29       169.66       172.02         9.53
         2048         1000       400.32       212.36       216.39        13.14
         4096         1000       454.48       244.21       248.78        15.48
         8192         1000       556.99       302.78       309.85        17.96
        16384         1000       780.64       432.32       440.53        20.93
        32768         1000      1235.39       688.49       700.68        21.95
        65536          640      3170.82      1696.08      1733.18        14.91
       131072          320      4312.55      2354.54      2410.58        18.77
       262144          160      8077.05      4433.14      4497.34        18.98
       524288           80     15645.42      8254.03      8342.42        11.40
      1048576           40     29596.16     15903.46     15993.15        14.38
      2097152           20     59619.46     30073.28     30090.67         1.81
      4194304           10    126542.00     64427.55     64415.62         3.57

#-----------------------------------------------------------------------------
# Benchmarking Ibcast 
# #processes = 16 
#-----------------------------------------------------------------------------
       #bytes #repetitions t_ovrl[usec] t_pure[usec]  t_CPU[usec]   overlap[%]
          512         1000       316.52       163.09       165.64         7.37
         1024         1000       347.87       178.25       183.28         7.45
         2048         1000       413.18       216.16       222.35        11.39
         4096         1000       474.25       249.78       256.09        12.35
         8192         1000       572.06       304.69       313.22        14.64
        16384         1000       794.01       429.77       439.73        17.17
        32768         1000      1288.65       711.70       723.74        20.28
        65536          640      3301.62      1736.13      1802.48        13.15
       131072          320      4904.55      2502.78      2630.68         8.70
       262144          160      9704.91      5149.49      5281.44        13.75
       524288           80     18054.63      8949.81      9076.30         0.00
      1048576           40     35342.68     18518.84     18561.79         9.36
      2097152           20     71482.35     38027.84     38042.31        12.06
      4194304           10    163937.83     79466.24     79370.95         0.00


# All processes entering MPI_Finalize

