 benchmarks to run Ibcast 
#------------------------------------------------------------
#    Intel (R) MPI Benchmarks 4.0 Update 1, MPI-NBC part  
#------------------------------------------------------------
# Date                  : Sun Sep 13 16:04:03 2015
# Machine               : x86_64
# System                : Linux
# Release               : 4.0.4-1.el6.elrepo.x86_64
# Version               : #1 SMP Mon May 18 12:45:35 EDT 2015
# MPI Version           : 3.0
# MPI Thread Environment: 

# New default behavior from Version 3.2 on:

# the number of iterations per message size is cut down 
# dynamically when a certain run time (per message size sample) 
# is expected to be exceeded. Time limit is defined by variable 
# "SECS_PER_SAMPLE" (=> IMB_settings.h) 
# or through the flag => -time 
  


# Calling sequence was: 

# /home/dycz0fx/program/imb/src/IMB-NBC Ibcast -msglen /home/dycz0fx/Lengths -npmin 16 -iter 1000
#                                       -time 20

# Message lengths were user defined
#
# MPI_Datatype                   :   MPI_BYTE 
# MPI_Datatype for reductions    :   MPI_FLOAT
# MPI_Op                         :   MPI_SUM  
#
#

# List of Benchmarks to run:

# Ibcast

#-----------------------------------------------------------------------------
# Benchmarking Ibcast 
# #processes = 16 
#-----------------------------------------------------------------------------
       #bytes #repetitions t_ovrl[usec] t_pure[usec]  t_CPU[usec]   overlap[%]
        65536          800      6046.15      2506.41      2512.25         0.00
       131072          656     11019.45      4347.68      4357.16         0.00
       262144          496     31753.96     15127.84     15146.60         0.00
       524288          320     60180.57     31121.97     31129.74         6.65
      1048576          192    126116.07     64928.54     64934.14         5.77
      2097152           96    241324.19    121729.56    121698.37         1.73
      4194304           48    373742.17    173379.01    173363.78         0.00


# All processes entering MPI_Finalize

