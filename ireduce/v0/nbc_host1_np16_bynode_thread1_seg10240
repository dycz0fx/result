 benchmarks to run Ireduce 
#------------------------------------------------------------
#    Intel (R) MPI Benchmarks 4.0 Update 1, MPI-NBC part  
#------------------------------------------------------------
# Date                  : Thu Aug 13 21:08:20 2015
# Machine               : x86_64
# System                : Linux
# Release               : 4.0.4-1.el6.elrepo.x86_64
# Version               : #1 SMP Mon May 18 12:45:35 EDT 2015
# MPI Version           : 3.0
# MPI Thread Environment: 

# New default behavior from Version 3.2 on:

# the number of iterations per message size is cut down 
# dynamically when a certain run time (per message size sample) 
# is expected to be exceeded. Time limit is defined by variable 
# "SECS_PER_SAMPLE" (=> IMB_settings.h) 
# or through the flag => -time 
  


# Calling sequence was: 

# /home/dycz0fx/program/imb/src/IMB-NBC Ireduce -msglen /home/dycz0fx/Lengths -npmin 16 -iter 1000
#                                      

# Message lengths were user defined
#
# MPI_Datatype                   :   MPI_BYTE 
# MPI_Datatype for reductions    :   MPI_FLOAT
# MPI_Op                         :   MPI_SUM  
#
#

# List of Benchmarks to run:

# Ireduce

#-----------------------------------------------------------------------------
# Benchmarking Ireduce 
# #processes = 16 
#-----------------------------------------------------------------------------
       #bytes #repetitions t_ovrl[usec] t_pure[usec]  t_CPU[usec]   overlap[%]
        65536          800      2394.49      1344.97      1379.71        23.93
       131072          656      4881.74      2671.99      2735.85        19.23
       262144          496      9704.93      5191.22      5553.83        18.73
       524288          320     19633.85     10225.49     10942.90        14.02
      1048576          192     48151.10     24855.49     25273.53         7.83
      2097152           96    127202.18     63926.12     66647.72         5.06
      4194304           48    391930.35    197422.58    202119.47         3.77


# All processes entering MPI_Finalize
